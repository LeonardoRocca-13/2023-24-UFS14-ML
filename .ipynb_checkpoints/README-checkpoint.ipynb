{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188970a6",
   "metadata": {},
   "source": [
    "Build our Docker image \"my-custom-sagemaker-training-image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1163f826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "#0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [sagemaker-training internal] load build definition from Dockerfile.train\n",
      "#1 transferring dockerfile: 708B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [sagemaker-training internal] load .dockerignore\n",
      "#2 transferring context: 2B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [sagemaker-training auth] sharing credentials for 763104351884.dkr.ecr.us-east-1.amazonaws.com\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [sagemaker-training internal] load metadata for 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.13.0-cpu-py310\n",
      "#4 DONE 0.1s\n",
      "\n",
      "#5 [sagemaker-training 1/3] FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.13.0-cpu-py310@sha256:2b0321e655591f49bd57a53817573e0371e942aa472119296cd3242585498d7e\n",
      "#5 CACHED\n",
      "\n",
      "#6 [sagemaker-training internal] load build context\n",
      "#6 transferring context: 23.65kB done\n",
      "#6 DONE 0.0s\n",
      "\n",
      "#7 [sagemaker-training 2/3] ADD ./src/train /opt/ml/code/\n",
      "#7 DONE 0.1s\n",
      "\n",
      "#8 [sagemaker-training 3/3] RUN pip install -r /opt/ml/code/requirements.txt\n",
      "#8 1.123 Collecting albumentations==1.3.1 (from -r /opt/ml/code/requirements.txt (line 1))\n",
      "#8 1.144   Downloading albumentations-1.3.1-py3-none-any.whl.metadata (34 kB)\n",
      "#8 1.164 Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/site-packages (from albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1)) (1.24.3)\n",
      "#8 1.165 Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/site-packages (from albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1)) (1.11.3)\n",
      "#8 1.293 Collecting scikit-image>=0.16.1 (from albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1))\n",
      "#8 1.299   Downloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "#8 1.307 Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/site-packages (from albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1)) (6.0.1)\n",
      "#8 1.326 Collecting qudida>=0.0.4 (from albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1))\n",
      "#8 1.330   Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
      "#8 1.339 Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/site-packages (from albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1)) (4.8.1.78)\n",
      "#8 1.366 Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1)) (1.3.2)\n",
      "#8 1.367 Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1)) (4.5.0)\n",
      "#8 1.487 Collecting networkx>=2.8 (from scikit-image>=0.16.1->albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1))\n",
      "#8 1.492   Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "#8 1.499 Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1)) (10.0.1)\n",
      "#8 1.500 Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1)) (2.32.0)\n",
      "#8 1.572 Collecting tifffile>=2022.8.12 (from scikit-image>=0.16.1->albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1))\n",
      "#8 1.576   Downloading tifffile-2023.9.26-py3-none-any.whl.metadata (30 kB)\n",
      "#8 1.584 Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1)) (23.2)\n",
      "#8 1.605 Collecting lazy_loader>=0.3 (from scikit-image>=0.16.1->albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1))\n",
      "#8 1.609   Downloading lazy_loader-0.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "#8 1.838 Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1)) (1.3.2)\n",
      "#8 1.839 Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r /opt/ml/code/requirements.txt (line 1)) (3.2.0)\n",
      "#8 1.867 Downloading albumentations-1.3.1-py3-none-any.whl (125 kB)\n",
      "#8 1.879    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.7/125.7 kB 12.7 MB/s eta 0:00:00\n",
      "#8 1.887 Downloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
      "#8 2.068    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/14.7 MB 69.2 MB/s eta 0:00:00\n",
      "#8 2.073 Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "#8 2.091 Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "#8 2.171    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 21.1 MB/s eta 0:00:00\n",
      "#8 2.179 Downloading tifffile-2023.9.26-py3-none-any.whl (222 kB)\n",
      "#8 2.242    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 222.9/222.9 kB 3.5 MB/s eta 0:00:00\n",
      "#8 4.408 Installing collected packages: tifffile, networkx, lazy_loader, scikit-image, qudida, albumentations\n",
      "#8 7.450 Successfully installed albumentations-1.3.1 lazy_loader-0.3 networkx-3.2.1 qudida-0.0.4 scikit-image-0.22.0 tifffile-2023.9.26\n",
      "#8 7.451 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "#8 DONE 8.2s\n",
      "\n",
      "#9 [sagemaker-training] exporting to image\n",
      "#9 exporting layers\n",
      "#9 exporting layers 0.6s done\n",
      "#9 writing image sha256:5d54f9263a1de671b95f83c6fe0d67ac6586e45eea2118247eb147490abf05ce done\n",
      "#9 naming to docker.io/library/my-custom-sagemaker-training-image done\n",
      "#9 DONE 0.7s\n",
      "DOCKER BUILD TERMINATED AT Fri Nov 24 16:52:04 UTC 2023\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# Hint: the ECR image we'll login for is the same we use as base image in the Dockerfile\n",
    "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.0-cpu-py3\n",
    "\n",
    "docker-compose build --no-cache sagemaker-training\n",
    "\n",
    "echo \"DOCKER BUILD TERMINATED AT $(date)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be951a",
   "metadata": {},
   "source": [
    "Using SageMaker Python SDK we can test our Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6186ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "##### ESTIMATOR FIT STARTED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: my-custom-sagemaker-training-image-2023-11-24-16-52-10-709\n",
      "INFO:sagemaker.local.image:'Docker Compose' is not installed. Proceeding to check for 'docker-compose' CLI.\n",
      "INFO:sagemaker.local.image:'Docker Compose' found using Docker Compose CLI.\n",
      "INFO:sagemaker.local.local_session:Starting training job\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-3yj86:\n",
      "    command: train\n",
      "    container_name: i01byybwfw-algo-1-3yj86\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: my-custom-sagemaker-training-image\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-3yj86\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmpyw4gctu_/algo-1-3yj86/output:/opt/ml/output\n",
      "    - /tmp/tmpyw4gctu_/algo-1-3yj86/input:/opt/ml/input\n",
      "    - /tmp/tmpyw4gctu_/algo-1-3yj86/output/data:/opt/ml/output/data\n",
      "    - /tmp/tmpyw4gctu_/model:/opt/ml/model\n",
      "    - /opt/ml/metadata:/opt/ml/metadata\n",
      "    - /home/ec2-user/SageMaker/2023-24-UFS14-ML/data/input:/opt/ml/input/data/training\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmpyw4gctu_/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Network sagemaker-local  Creating\n",
      " Network sagemaker-local  Created\n",
      " Container i01byybwfw-algo-1-3yj86  Creating\n",
      " Container i01byybwfw-algo-1-3yj86  Created\n",
      "Attaching to i01byybwfw-algo-1-3yj86\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:15,808 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:15,810 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:15,810 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:15,823 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:15,826 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "i01byybwfw-algo-1-3yj86  | /usr/local/bin/python3.10 -m pip install -r requirements.txt\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: albumentations==1.3.1 in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.3.1)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/site-packages (from albumentations==1.3.1->-r requirements.txt (line 1)) (1.24.3)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/site-packages (from albumentations==1.3.1->-r requirements.txt (line 1)) (1.11.3)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/site-packages (from albumentations==1.3.1->-r requirements.txt (line 1)) (0.22.0)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/site-packages (from albumentations==1.3.1->-r requirements.txt (line 1)) (6.0.1)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/site-packages (from albumentations==1.3.1->-r requirements.txt (line 1)) (0.0.4)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/site-packages (from albumentations==1.3.1->-r requirements.txt (line 1)) (4.8.1.78)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations==1.3.1->-r requirements.txt (line 1)) (1.3.2)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations==1.3.1->-r requirements.txt (line 1)) (4.5.0)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r requirements.txt (line 1)) (3.2.1)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r requirements.txt (line 1)) (10.0.1)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r requirements.txt (line 1)) (2.32.0)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r requirements.txt (line 1)) (2023.9.26)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r requirements.txt (line 1)) (23.2)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: lazy_loader>=0.3 in /usr/local/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r requirements.txt (line 1)) (0.3)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r requirements.txt (line 1)) (1.3.2)\n",
      "i01byybwfw-algo-1-3yj86  | Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r requirements.txt (line 1)) (3.2.0)\n",
      "i01byybwfw-algo-1-3yj86  | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,721 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,721 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,722 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,723 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,734 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,738 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,739 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,750 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,753 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,754 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,765 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,768 sagemaker-training-toolkit INFO     Invoking user script\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | Training Env:\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | {\n",
      "i01byybwfw-algo-1-3yj86  |     \"additional_framework_parameters\": {},\n",
      "i01byybwfw-algo-1-3yj86  |     \"channel_input_dirs\": {\n",
      "i01byybwfw-algo-1-3yj86  |         \"training\": \"/opt/ml/input/data/training\"\n",
      "i01byybwfw-algo-1-3yj86  |     },\n",
      "i01byybwfw-algo-1-3yj86  |     \"current_host\": \"algo-1-3yj86\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"current_instance_group\": \"homogeneousCluster\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"current_instance_group_hosts\": [],\n",
      "i01byybwfw-algo-1-3yj86  |     \"current_instance_type\": \"local\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"distribution_hosts\": [\n",
      "i01byybwfw-algo-1-3yj86  |         \"algo-1-3yj86\"\n",
      "i01byybwfw-algo-1-3yj86  |     ],\n",
      "i01byybwfw-algo-1-3yj86  |     \"distribution_instance_groups\": [],\n",
      "i01byybwfw-algo-1-3yj86  |     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"hosts\": [\n",
      "i01byybwfw-algo-1-3yj86  |         \"algo-1-3yj86\"\n",
      "i01byybwfw-algo-1-3yj86  |     ],\n",
      "i01byybwfw-algo-1-3yj86  |     \"hyperparameters\": {\n",
      "i01byybwfw-algo-1-3yj86  |         \"epochs\": 1\n",
      "i01byybwfw-algo-1-3yj86  |     },\n",
      "i01byybwfw-algo-1-3yj86  |     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"input_data_config\": {\n",
      "i01byybwfw-algo-1-3yj86  |         \"training\": {\n",
      "i01byybwfw-algo-1-3yj86  |             \"TrainingInputMode\": \"File\"\n",
      "i01byybwfw-algo-1-3yj86  |         }\n",
      "i01byybwfw-algo-1-3yj86  |     },\n",
      "i01byybwfw-algo-1-3yj86  |     \"input_dir\": \"/opt/ml/input\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"instance_groups\": [],\n",
      "i01byybwfw-algo-1-3yj86  |     \"instance_groups_dict\": {},\n",
      "i01byybwfw-algo-1-3yj86  |     \"is_hetero\": false,\n",
      "i01byybwfw-algo-1-3yj86  |     \"is_master\": true,\n",
      "i01byybwfw-algo-1-3yj86  |     \"is_modelparallel_enabled\": null,\n",
      "i01byybwfw-algo-1-3yj86  |     \"is_smddpmprun_installed\": false,\n",
      "i01byybwfw-algo-1-3yj86  |     \"is_smddprun_installed\": false,\n",
      "i01byybwfw-algo-1-3yj86  |     \"job_name\": \"my-custom-sagemaker-training-image-2023-11-24-16-52-10-709\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"log_level\": 20,\n",
      "i01byybwfw-algo-1-3yj86  |     \"master_hostname\": \"algo-1-3yj86\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"model_dir\": \"/opt/ml/model\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"module_dir\": \"/opt/ml/code\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"module_name\": \"my-custom-training-script\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"network_interface_name\": \"eth0\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"num_cpus\": 16,\n",
      "i01byybwfw-algo-1-3yj86  |     \"num_gpus\": 0,\n",
      "i01byybwfw-algo-1-3yj86  |     \"num_neurons\": 0,\n",
      "i01byybwfw-algo-1-3yj86  |     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"output_dir\": \"/opt/ml/output\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "i01byybwfw-algo-1-3yj86  |     \"resource_config\": {\n",
      "i01byybwfw-algo-1-3yj86  |         \"current_host\": \"algo-1-3yj86\",\n",
      "i01byybwfw-algo-1-3yj86  |         \"hosts\": [\n",
      "i01byybwfw-algo-1-3yj86  |             \"algo-1-3yj86\"\n",
      "i01byybwfw-algo-1-3yj86  |         ]\n",
      "i01byybwfw-algo-1-3yj86  |     },\n",
      "i01byybwfw-algo-1-3yj86  |     \"user_entry_point\": \"my-custom-training-script.py\"\n",
      "i01byybwfw-algo-1-3yj86  | }\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | Environment variables:\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | SM_HOSTS=[\"algo-1-3yj86\"]\n",
      "i01byybwfw-algo-1-3yj86  | SM_NETWORK_INTERFACE_NAME=eth0\n",
      "i01byybwfw-algo-1-3yj86  | SM_HPS={\"epochs\":1}\n",
      "i01byybwfw-algo-1-3yj86  | SM_USER_ENTRY_POINT=my-custom-training-script.py\n",
      "i01byybwfw-algo-1-3yj86  | SM_FRAMEWORK_PARAMS={}\n",
      "i01byybwfw-algo-1-3yj86  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-3yj86\",\"hosts\":[\"algo-1-3yj86\"]}\n",
      "i01byybwfw-algo-1-3yj86  | SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "i01byybwfw-algo-1-3yj86  | SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "i01byybwfw-algo-1-3yj86  | SM_CHANNELS=[\"training\"]\n",
      "i01byybwfw-algo-1-3yj86  | SM_CURRENT_HOST=algo-1-3yj86\n",
      "i01byybwfw-algo-1-3yj86  | SM_CURRENT_INSTANCE_TYPE=local\n",
      "i01byybwfw-algo-1-3yj86  | SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "i01byybwfw-algo-1-3yj86  | SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "i01byybwfw-algo-1-3yj86  | SM_INSTANCE_GROUPS=[]\n",
      "i01byybwfw-algo-1-3yj86  | SM_INSTANCE_GROUPS_DICT={}\n",
      "i01byybwfw-algo-1-3yj86  | SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "i01byybwfw-algo-1-3yj86  | SM_IS_HETERO=false\n",
      "i01byybwfw-algo-1-3yj86  | SM_MODULE_NAME=my-custom-training-script\n",
      "i01byybwfw-algo-1-3yj86  | SM_LOG_LEVEL=20\n",
      "i01byybwfw-algo-1-3yj86  | SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "i01byybwfw-algo-1-3yj86  | SM_INPUT_DIR=/opt/ml/input\n",
      "i01byybwfw-algo-1-3yj86  | SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "i01byybwfw-algo-1-3yj86  | SM_OUTPUT_DIR=/opt/ml/output\n",
      "i01byybwfw-algo-1-3yj86  | SM_NUM_CPUS=16\n",
      "i01byybwfw-algo-1-3yj86  | SM_NUM_GPUS=0\n",
      "i01byybwfw-algo-1-3yj86  | SM_NUM_NEURONS=0\n",
      "i01byybwfw-algo-1-3yj86  | SM_MODEL_DIR=/opt/ml/model\n",
      "i01byybwfw-algo-1-3yj86  | SM_MODULE_DIR=/opt/ml/code\n",
      "i01byybwfw-algo-1-3yj86  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-3yj86\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-3yj86\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-3yj86\"],\"hyperparameters\":{\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"my-custom-sagemaker-training-image-2023-11-24-16-52-10-709\",\"log_level\":20,\"master_hostname\":\"algo-1-3yj86\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"my-custom-training-script\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-3yj86\",\"hosts\":[\"algo-1-3yj86\"]},\"user_entry_point\":\"my-custom-training-script.py\"}\n",
      "i01byybwfw-algo-1-3yj86  | SM_USER_ARGS=[\"--epochs\",\"1\"]\n",
      "i01byybwfw-algo-1-3yj86  | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "i01byybwfw-algo-1-3yj86  | SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "i01byybwfw-algo-1-3yj86  | SM_HP_EPOCHS=1\n",
      "i01byybwfw-algo-1-3yj86  | PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python310.zip:/usr/local/lib/python3.10:/usr/local/lib/python3.10/lib-dynload:/usr/local/lib/python3.10/site-packages\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | Invoking script with the following command:\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | /usr/local/bin/python3.10 my-custom-training-script.py --epochs 1\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:18,768 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "i01byybwfw-algo-1-3yj86  | INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:23.061714: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\n",
      "i01byybwfw-algo-1-3yj86  | op: \"TensorDataset\"\n",
      "i01byybwfw-algo-1-3yj86  | input: \"Placeholder/_0\"\n",
      "i01byybwfw-algo-1-3yj86  | attr {\n",
      "i01byybwfw-algo-1-3yj86  |   key: \"Toutput_types\"\n",
      "i01byybwfw-algo-1-3yj86  |   value {\n",
      "i01byybwfw-algo-1-3yj86  |     list {\n",
      "i01byybwfw-algo-1-3yj86  |       type: DT_INT32\n",
      "i01byybwfw-algo-1-3yj86  |     }\n",
      "i01byybwfw-algo-1-3yj86  |   }\n",
      "i01byybwfw-algo-1-3yj86  | }\n",
      "i01byybwfw-algo-1-3yj86  | attr {\n",
      "i01byybwfw-algo-1-3yj86  |   key: \"_cardinality\"\n",
      "i01byybwfw-algo-1-3yj86  |   value {\n",
      "i01byybwfw-algo-1-3yj86  |     i: 1\n",
      "i01byybwfw-algo-1-3yj86  |   }\n",
      "i01byybwfw-algo-1-3yj86  | }\n",
      "i01byybwfw-algo-1-3yj86  | attr {\n",
      "i01byybwfw-algo-1-3yj86  |   key: \"metadata\"\n",
      "i01byybwfw-algo-1-3yj86  |   value {\n",
      "i01byybwfw-algo-1-3yj86  |     s: \"\\n\\017TensorDataset:0\"\n",
      "i01byybwfw-algo-1-3yj86  |   }\n",
      "i01byybwfw-algo-1-3yj86  | }\n",
      "i01byybwfw-algo-1-3yj86  | attr {\n",
      "i01byybwfw-algo-1-3yj86  |   key: \"output_shapes\"\n",
      "i01byybwfw-algo-1-3yj86  |   value {\n",
      "i01byybwfw-algo-1-3yj86  |     list {\n",
      "i01byybwfw-algo-1-3yj86  |       shape {\n",
      "i01byybwfw-algo-1-3yj86  |       }\n",
      "i01byybwfw-algo-1-3yj86  |     }\n",
      "i01byybwfw-algo-1-3yj86  |   }\n",
      "i01byybwfw-algo-1-3yj86  | }\n",
      "i01byybwfw-algo-1-3yj86  | experimental_type {\n",
      "i01byybwfw-algo-1-3yj86  |   type_id: TFT_PRODUCT\n",
      "i01byybwfw-algo-1-3yj86  |   args {\n",
      "i01byybwfw-algo-1-3yj86  |     type_id: TFT_DATASET\n",
      "i01byybwfw-algo-1-3yj86  |     args {\n",
      "i01byybwfw-algo-1-3yj86  |       type_id: TFT_PRODUCT\n",
      "i01byybwfw-algo-1-3yj86  |       args {\n",
      "i01byybwfw-algo-1-3yj86  |         type_id: TFT_TENSOR\n",
      "i01byybwfw-algo-1-3yj86  |         args {\n",
      "i01byybwfw-algo-1-3yj86  |           type_id: TFT_INT32\n",
      "i01byybwfw-algo-1-3yj86  |         }\n",
      "i01byybwfw-algo-1-3yj86  |       }\n",
      "i01byybwfw-algo-1-3yj86  |     }\n",
      "i01byybwfw-algo-1-3yj86  |   }\n",
      "i01byybwfw-algo-1-3yj86  | }\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:23.136104: W tensorflow/core/framework/dataset.cc:956] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "i01byybwfw-algo-1-3yj86  | Epoch 1/20\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:29.400857: W tensorflow/core/framework/op_kernel.cc:1816] UNKNOWN: FileNotFoundError: [Errno 2] No such file or directory: 'resources/training'\n",
      "i01byybwfw-algo-1-3yj86  | Traceback (most recent call last):\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  |   File \"/usr/local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "i01byybwfw-algo-1-3yj86  |     ret = func(*args)\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  |   File \"/usr/local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "i01byybwfw-algo-1-3yj86  |     return func(*args, **kwargs)\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  |   File \"/usr/local/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "i01byybwfw-algo-1-3yj86  |     values = next(generator_state.get_iterator(iterator_id))\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  |   File \"/opt/ml/code/libraries/Image_generator.py\", line 18, in Image_generator\n",
      "i01byybwfw-algo-1-3yj86  |     patients = os.listdir(f'resources/{split}')\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | FileNotFoundError: [Errno 2] No such file or directory: 'resources/training'\n",
      "i01byybwfw-algo-1-3yj86  | Traceback (most recent call last):\n",
      "i01byybwfw-algo-1-3yj86  | File \"/opt/ml/code/my-custom-training-script.py\", line 15, in <module>\n",
      "i01byybwfw-algo-1-3yj86  | Train_FCNN_Model(\n",
      "i01byybwfw-algo-1-3yj86  |   File \"/opt/ml/code/libraries/Train_model_pipeline.py\", line 77, in Train_FCNN_Model\n",
      "i01byybwfw-algo-1-3yj86  | history = fcnn.fit(\n",
      "i01byybwfw-algo-1-3yj86  |   File \"/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
      "i01byybwfw-algo-1-3yj86  | raise e.with_traceback(filtered_tb) from None\n",
      "i01byybwfw-algo-1-3yj86  |   File \"/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
      "i01byybwfw-algo-1-3yj86  | tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "i01byybwfw-algo-1-3yj86  | tensorflow.python.framework.errors_impl.UnknownError: Graph execution error:\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | FileNotFoundError: [Errno 2] No such file or directory: 'resources/training'\n",
      "i01byybwfw-algo-1-3yj86  | Traceback (most recent call last):\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  |   File \"/usr/local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "i01byybwfw-algo-1-3yj86  |     ret = func(*args)\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  |   File \"/usr/local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "i01byybwfw-algo-1-3yj86  |     return func(*args, **kwargs)\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  |   File \"/usr/local/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "i01byybwfw-algo-1-3yj86  |     values = next(generator_state.get_iterator(iterator_id))\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  |   File \"/opt/ml/code/libraries/Image_generator.py\", line 18, in Image_generator\n",
      "i01byybwfw-algo-1-3yj86  |     patients = os.listdir(f'resources/{split}')\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | FileNotFoundError: [Errno 2] No such file or directory: 'resources/training'\n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | \n",
      "i01byybwfw-algo-1-3yj86  | \t [[{{node PyFunc}}]]\n",
      "i01byybwfw-algo-1-3yj86  | \t [[MultiDeviceIteratorGetNextFromShard]]\n",
      "i01byybwfw-algo-1-3yj86  | \t [[RemoteCall]]\n",
      "i01byybwfw-algo-1-3yj86  | \t [[IteratorGetNextAsOptional]] [Op:__inference_train_function_7839]\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:30,096 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:30,096 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:30,096 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:30,096 sagemaker-training-toolkit ERROR    NotFoundError:\n",
      "i01byybwfw-algo-1-3yj86  | ExitCode 1\n",
      "i01byybwfw-algo-1-3yj86  | ErrorMessage \"2023-11-24 16:52:29.400857: W tensorflow/core/framework/op_kernel.cc:1816] UNKNOWN: FileNotFoundError: [Errno 2] No such file or directory: 'resources/training'\n",
      "i01byybwfw-algo-1-3yj86  |  Traceback (most recent call last)\n",
      "i01byybwfw-algo-1-3yj86  |  \n",
      "i01byybwfw-algo-1-3yj86  |  File \"/usr/local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "i01byybwfw-algo-1-3yj86  |  ret = func(*args)\n",
      "i01byybwfw-algo-1-3yj86  |  File \"/usr/local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "i01byybwfw-algo-1-3yj86  |  return func(*args, **kwargs)\n",
      "i01byybwfw-algo-1-3yj86  |  File \"/usr/local/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "i01byybwfw-algo-1-3yj86  |  values = next(generator_state.get_iterator(iterator_id))\n",
      "i01byybwfw-algo-1-3yj86  |  File \"/opt/ml/code/libraries/Image_generator.py\", line 18, in Image_generator\n",
      "i01byybwfw-algo-1-3yj86  |  patients = os.listdir(f'resources/{split}')\n",
      "i01byybwfw-algo-1-3yj86  |  FileNotFoundError: [Errno 2] No such file or directory: 'resources/training'\n",
      "i01byybwfw-algo-1-3yj86  |  File \"/opt/ml/code/my-custom-training-script.py\", line 15, in <module>\n",
      "i01byybwfw-algo-1-3yj86  |  Train_FCNN_Model(\n",
      "i01byybwfw-algo-1-3yj86  |  File \"/opt/ml/code/libraries/Train_model_pipeline.py\", line 77, in Train_FCNN_Model\n",
      "i01byybwfw-algo-1-3yj86  |  history = fcnn.fit(\n",
      "i01byybwfw-algo-1-3yj86  |  File \"/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
      "i01byybwfw-algo-1-3yj86  |  raise e.with_traceback(filtered_tb) from None\n",
      "i01byybwfw-algo-1-3yj86  |  File \"/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
      "i01byybwfw-algo-1-3yj86  |  tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "i01byybwfw-algo-1-3yj86  |  tensorflow.python.framework.errors_impl.UnknownError: Graph execution error\n",
      "i01byybwfw-algo-1-3yj86  |  \t [[{{node PyFunc}}]]\n",
      "i01byybwfw-algo-1-3yj86  |  \t [[MultiDeviceIteratorGetNextFromShard]]\n",
      "i01byybwfw-algo-1-3yj86  |  \t [[RemoteCall]]\n",
      "i01byybwfw-algo-1-3yj86  |  \t [[IteratorGetNextAsOptional]] [Op:__inference_train_function_7839]\"\n",
      "i01byybwfw-algo-1-3yj86  | Command \"/usr/local/bin/python3.10 my-custom-training-script.py --epochs 1\"\n",
      "i01byybwfw-algo-1-3yj86  | 2023-11-24 16:52:30,097 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:creating /tmp/tmpyw4gctu_/artifacts/output/data\n",
      "INFO:root:copying /tmp/tmpyw4gctu_/algo-1-3yj86/output/data/logs-training.txt -> /tmp/tmpyw4gctu_/artifacts/output/data\n",
      "INFO:root:copying /tmp/tmpyw4gctu_/algo-1-3yj86/output/failure -> /tmp/tmpyw4gctu_/artifacts/output\n",
      "INFO:root:copying /tmp/tmpyw4gctu_/compressed_artifacts/model.tar.gz -> /home/ec2-user/SageMaker/2023-24-UFS14-ML/data/output\n",
      "INFO:root:copying /tmp/tmpyw4gctu_/compressed_artifacts/output.tar.gz -> /home/ec2-user/SageMaker/2023-24-UFS14-ML/data/output\n",
      "WARNING:sagemaker.local.image:Failed to delete: /tmp/tmpyw4gctu_/algo-1-3yj86 Please remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i01byybwfw-algo-1-3yj86 exited with code 1\n",
      "Aborting on container exit...\n",
      " Container i01byybwfw-algo-1-3yj86  Stopping\n",
      " Container i01byybwfw-algo-1-3yj86  Stopped\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run: ['docker-compose', '-f', '/tmp/tmpyw4gctu_/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/local/image.py:296\u001b[0m, in \u001b[0;36m_SageMakerContainer.train\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[43m_stream_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# _stream_output() doesn't have the command line. We will handle the exception\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# which contains the exit code and append the command line to it.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/local/image.py:984\u001b[0m, in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exit_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess exited with code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m exit_code)\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exit_code\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Process exited with code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m      9\u001b[0m estimator\u001b[38;5;241m=\u001b[39mEstimator(\n\u001b[1;32m     10\u001b[0m     image_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy-custom-sagemaker-training-image\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile://\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/data/output\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m##### ESTIMATOR FIT STARTED\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile://\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/data/input/my-input-csv-file.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m##### ESTIMATOR FIT COMPLETED\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1319\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_for_training(job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[1;32m   1318\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m \u001b[43m_TrainingJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2382\u001b[0m, in \u001b[0;36m_TrainingJob.start_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a new Amazon SageMaker training job from the estimator.\u001b[39;00m\n\u001b[1;32m   2358\u001b[0m \n\u001b[1;32m   2359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;124;03m    all information about the started training job.\u001b[39;00m\n\u001b[1;32m   2379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2380\u001b[0m train_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_train_args(estimator, inputs, experiment_config)\n\u001b[0;32m-> 2382\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(estimator\u001b[38;5;241m.\u001b[39msagemaker_session, estimator\u001b[38;5;241m.\u001b[39m_current_job_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:941\u001b[0m, in \u001b[0;36mSession.train\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, training_image_config, infra_check_config, container_entry_point, container_arguments, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    938\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_training_job(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n\u001b[0;32m--> 941\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:5618\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   5601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   5602\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5603\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5606\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   5607\u001b[0m ):\n\u001b[1;32m   5608\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   5609\u001b[0m \n\u001b[1;32m   5610\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5616\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   5617\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:939\u001b[0m, in \u001b[0;36mSession.train.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    937\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating training-job with name: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, job_name)\n\u001b[1;32m    938\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/local/local_session.py:203\u001b[0m, in \u001b[0;36mLocalSagemakerClient.create_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, Environment, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    202\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training job\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m \u001b[43mtraining_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mInputDataConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOutputDataConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEnvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainingJobName\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m LocalSagemakerClient\u001b[38;5;241m.\u001b[39m_training_jobs[TrainingJobName] \u001b[38;5;241m=\u001b[39m training_job\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/local/entities.py:243\u001b[0m, in \u001b[0;36m_LocalTrainingJob.start\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_TRAINING\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment \u001b[38;5;241m=\u001b[39m environment\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_artifacts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_data_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_COMPLETED\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/local/image.py:301\u001b[0m, in \u001b[0;36m_SageMakerContainer.train\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# _stream_output() doesn't have the command line. We will handle the exception\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# which contains the exit code and append the command line to it.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (compose_command, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     artifacts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_artifacts(compose_data, output_data_config, job_name)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run: ['docker-compose', '-f', '/tmp/tmpyw4gctu_/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "import os\n",
    "\n",
    "role=get_execution_role()\n",
    "\n",
    "hyperparameters={'epochs': 1}\n",
    "\n",
    "estimator=Estimator(\n",
    "    image_uri='my-custom-sagemaker-training-image',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='local',\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path='file://{}/data/output'.format(os.getcwd())\n",
    ")\n",
    "\n",
    "print('##### ESTIMATOR FIT STARTED')\n",
    "estimator.fit('file://{}/data/input/'.format(os.getcwd()))\n",
    "print('##### ESTIMATOR FIT COMPLETED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34137b3a",
   "metadata": {},
   "source": [
    "NB: if you encountered an error related to `network sagemaker-local was found but has incorrect label com.docker.compose.network set to \"\"` run the following command in the terminal and retry the above cell\n",
    "`docker network prune --force`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# Extracting local training archives to see the results\n",
    "\n",
    "tar -xvf $PWD/data/output/model.tar.gz -C $PWD/data/output/model\n",
    "tar -xvf $PWD/data/output/output.tar.gz -C $PWD/data/output\n",
    "\n",
    "echo \"Check the above files in the $PWD/data/output directory!!!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5bb446",
   "metadata": {},
   "source": [
    "As our image works as expected we can build it again with the right ECR image URI and push it to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# Specify an image name\n",
    "image_name=my-custom-sagemaker-training-image\n",
    "echo \"image_name: ${image_name} ######################\"\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "echo \"account: ${account} ######################\"\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "echo \"region: ${region} ######################\"\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image_name}:latest\"\n",
    "echo \"fullname: ${fullname} ######################\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${image_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "aws ecr create-repository --repository-name \"${image_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Log into Docker\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${image_name} -f Dockerfile.train .\n",
    "docker tag ${image_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}\n",
    "\n",
    "echo \"Docker push ended at $(date)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d81a4e",
   "metadata": {},
   "source": [
    "NB: if the last command \"docker push\" remain pending check README.md \"AWS ECR IAM policies\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7dcc86",
   "metadata": {},
   "source": [
    "Before executing a training job on SageMaker we need to move our input data to AWS S3.\n",
    "Obv. we also need an S3 bucket first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aee06b",
   "metadata": {},
   "source": [
    "Create an S3 bucket using AWS CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c4dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random AWS S3 bucket name sharing the name between sh/bash and other Python cells.\n",
    "# NB: need to be executed only the first time you want to create the AWS S3 bucket\n",
    "import random\n",
    "\n",
    "bucket_name='a-random-bucket-name-{}'.format(random.randint(0, 1000000))\n",
    "\n",
    "%set_env AWS_S3_BUCKET_NAME=$bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b780b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# NB: need to be executed only the first time you want to create the AWS S3 bucket\n",
    "aws s3api create-bucket --bucket $AWS_S3_BUCKET_NAME --region $(aws configure get region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b80db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker\n",
    "\n",
    "url = 'file://{}/data/input/my-input-csv-file.csv'.format(os.getcwd())\n",
    "df_demo = pd.read_csv(url,',')\n",
    "\n",
    "prefix='demo'\n",
    "train_file='demo_train.csv'\n",
    "test_file='demo_test.csv'\n",
    "validate_file='demo_validate.csv'\n",
    "whole_file='demo.csv'\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "train, test_and_validate = train_test_split(df_demo, \n",
    "                                            test_size=0.2, \n",
    "                                            random_state=42, \n",
    "                                            stratify=df_demo['quality'])\n",
    "\n",
    "test, validate = train_test_split(test_and_validate, \n",
    "                                  test_size=0.5, \n",
    "                                  random_state=42, \n",
    "                                  stratify=test_and_validate['quality'])\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(validate.shape)\n",
    "\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket_name).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "upload_s3_csv(train_file, 'train', train)\n",
    "upload_s3_csv(test_file, 'test', test)\n",
    "upload_s3_csv(validate_file, 'validate', validate)\n",
    "\n",
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket_name, prefix, train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket_name, prefix, validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49434a8f",
   "metadata": {},
   "source": [
    "As we have pushed our Docker image to ECR and uploaded our input data to AWS S3 we can use it with a training job on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58844c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "client=boto3.client('sts')\n",
    "account=client.get_caller_identity()['Account']\n",
    "\n",
    "my_session=boto3.session.Session()\n",
    "region=my_session.region_name\n",
    "\n",
    "image_name='my-custom-sagemaker-training-image'\n",
    "ecr_image='{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, image_name)\n",
    "print('###### ecr_image is: {}'.format(ecr_image))\n",
    "\n",
    "estimator=Estimator(\n",
    "    image_uri=ecr_image,\n",
    "    role=get_execution_role(),\n",
    "    base_job_name='custom-docker-image-for-training',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p2.xlarge',\n",
    "    output_path='s3://{}'.format(bucket_name)\n",
    ")\n",
    "\n",
    "# start training\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912923b2",
   "metadata": {},
   "source": [
    "Before deploy our model we need to test it locally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f667f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# See README.md for explanation\n",
    "# Hint: the ECR image we'll login for is the same we use as base image in the Dockerfile\n",
    "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.5.0-cpu-py3\n",
    "\n",
    "docker-compose build sagemaker-inference\n",
    "\n",
    "echo \"DOCKER BUILD TERMINATED AT $(date)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefec175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# create our sagemaker-inference container\n",
    "docker-compose create sagemaker-inference\n",
    "\n",
    "# create our sagemaker-inference container\n",
    "docker-compose start sagemaker-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4263e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# read logs of our sagemaker-inference container\n",
    "cat $PWD/data/output-compose/data/logs-inference.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784bd7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "echo \"GET /ping BELOW\"\n",
    "# call our ping endpoint\n",
    "curl localhost:8080/ping\n",
    "echo \"\"\n",
    "\n",
    "echo \"POST /invocations BELOW\"\n",
    "# call our inference endpoint\n",
    "curl -X post localhost:8080/invocations\n",
    "echo \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose stop sagemaker-inference\n",
    "docker-compose rm sagemaker-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7cbf2",
   "metadata": {},
   "source": [
    "Let's go deploy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63aa31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# Specify an image name\n",
    "image_name=my-custom-sagemaker-inference-image\n",
    "echo \"image_name: ${image_name} ######################\"\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "echo \"account: ${account} ######################\"\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "echo \"region: ${region} ######################\"\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image_name}:latest\"\n",
    "echo \"fullname: ${fullname} ######################\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${image_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "aws ecr create-repository --repository-name \"${image_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Log into Docker\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${image_name} -f Dockerfile.inference .\n",
    "docker tag ${image_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}\n",
    "\n",
    "echo \"Docker push ended at $(date)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc2a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import datetime\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "aws_region = my_session.region_name\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
    "\n",
    "sagemaker_role = get_execution_role()\n",
    "\n",
    "model_name = 'training-2023-11-23-18-45-31-074'\n",
    "\n",
    "# Create model\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = sagemaker_role,\n",
    "    # cambiare immagine!\n",
    "    PrimaryContainer = {\n",
    "        'Image': '590460693729.dkr.ecr.us-east-1.amazonaws.com/my-custom-sagemaker-inference-image:latest',\n",
    "        'ModelDataUrl': 's3://a-random-bucket-name-751357/custom-docker-image-for-training-2023-11-23-18-45-31-074/output/model.tar.gz',\n",
    "    })\n",
    "\n",
    "\n",
    "# Create an endpoint config name. Here we create one based on the date  \n",
    "# so it we can search endpoints based on creation time.\n",
    "endpoint_config_name = 'my-first-custom-endpoint-config-name'\n",
    "\n",
    "instance_type = 'ml.p2.xlarge'\n",
    "\n",
    "endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    # You will specify this name in a CreateEndpoint request.\n",
    "    # List of ProductionVariant objects, one for each model that you want to host at this endpoint.\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\", # The name of the production variant.\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": instance_type, # Specify the compute instance type.\n",
    "            \"InitialInstanceCount\": 1 # Number of instances to launch initially.\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Created EndpointConfig: {endpoint_config_response['EndpointConfigArn']}\")\n",
    "\n",
    "# The name of the endpoint. The name must be unique within an AWS Region in your AWS account.\n",
    "endpoint_name = 'my-first-custom-endpoint'\n",
    "\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(\n",
    "                                            EndpointName=endpoint_name, \n",
    "                                            EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137bd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# Let's try our inference!!!\n",
    "aws sagemaker-runtime invoke-endpoint --endpoint-name 'my-first-custom-endpoint' --body '{}' inference-response.json\n",
    "\n",
    "cat inference-response.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
